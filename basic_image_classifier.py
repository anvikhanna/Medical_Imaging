# -*- coding: utf-8 -*-
"""shantanu_1class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VzMN417I1RYRQdA9JlTZ4CvEn9klVfNQ
"""

from tensorflow import keras
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

dataset = keras.datasets.fashion_mnist

#print(dataset.__dict__)

#print(dir(dataset))

#data = dataset.load_data

#print(data)

data=dataset.load_data()

#print(data)

print(dir(data))

#print(data)

(xtrain,ytrain),(xtest,ytest)=data

print(data)

class_names= ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

print(xtrain.shape)
print(ytrain.shape)
print(xtest.shape)
print(ytest.shape)

plt.figure()

plt.figure(figsize=(28,28))

plt.imshow(xtrain[0])
plt.show()

plt.imshow(xtrain[273])
plt.colorbar()
plt.show()

"""this is a text"""

#Flatten -> Dense(ReLU...example depending on ouput you want) -> Dense(softmax)

#normalization
xtrain = xtrain/255
xtest = xtest/255
HP_VECTORS = 128
HP_EPOCHS = 10
#when data can be normalized using normal operations then dont use formula

#Flatten -> Dense(ReLU) -> Dense(softmax)
layerl= keras.layers.Flatten(input_shape=(28,28))
#from layer2 we cant change or manipulate(check) the input or we cant change the number of input after layer1
layer2= keras.layers.Dense(128,activation=tf.nn.relu)
#tensorflow.neuralnetworks=tf.nn
# 128 we have taken randomly .. we take random number in 2 ki power and see which gives the best result and it(128) represents how many parts you want your input(28,28) to be broken into, also there will be 128 biases
layer3= keras.layers.Dense(10,activation=tf.nn.softmax)
#layer3 is the output layer, hence 10 will be the lines we want in our ouput, as we have 10 classifications..Trouser, Pullover, Dress,etc..we will take 10 output lines. 
#Softmax will create a list of probability distributions of our classification.

#model= keras.Sequential([layer1,layer2,layer3])
#model.compile(loss=,metrics=,optimization=)

model = keras.Sequential([layerl,layer2,layer3])

model.compile(loss='sparse_categorical_crossentropy',metrics=['accuracy'],optimizer='adam')
print(model.summary())

(784*128)+128

"""the 128 added are 128 biases, one for each"""

(128*10)+10

"""the 10 added are 10 biases, one for each"""

history=model.fit(xtrain,ytrain,epochs=HP_EPOCHS,verbose=0)

"""history is a variable that we are using to store history of each epochs or step

verbose is used to display output, verbose=0 for no ouput
"""

print(dir(history))

print(history.history)

#can read it as a column, pass it to matplotlib for the graph

loss_data=history.history['loss']
acc_data=history.history['acc']

print(loss_data)
print(acc_data)

predictions=model.predict(xtest)
print(predictions[3])

print(np.argmax(predictions[3]))

mistakes=0
for i in range(10000):
  if mistakes>5:
    break
  if(np.argmax(predictions[i]) !=ytest[i]):
    mistakes += 1
    plt.figure()#empty space for the graph
    plt.xlabel('Real output: '+class_names[ytest[i]])
    plt.ylabel('Predicted mistakes: '+class_names[np.argmax(predictions[i])])
    plt.imshow(xtest[i])
#np.argmax(predictions[i]) basically sees in the probability distribution created by softmax which has the highest value. Eg. for 3rd image in xtest, there will be some probability for each type, like tshirt:0.2, snadal:0.1,sneaker:03...etc, so np.argmax will take the highest value and display class_name corresponding to it.

